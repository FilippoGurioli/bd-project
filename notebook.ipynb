{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc897058-f991-4ebe-9f6b-d163fdd0ddb2",
   "metadata": {},
   "source": [
    "# Big Data Project Notebook\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Name: NYC Taxi & Limousine Commission (TLC) Trip Record Data\n",
    "Source link: [NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page?utm_source=chatgpt.com)\n",
    "\n",
    "## Main Job\n",
    "\n",
    "**Objective**: Analyze how tip generosity varies by pickup location and time of day, and identify the top pickup zones with the most generous passengers.\n",
    "\n",
    "### Plan\n",
    "\n",
    "#### Setup\n",
    "\n",
    "- clean up and select relevant columns\n",
    "- create derived columns (tip percentage, hour of day)\n",
    "\n",
    "#### Shuffles\n",
    "\n",
    "- join zones id with zone lookup table\n",
    "- aggregate per location and hour\n",
    "- aggregate per average tip percentage\n",
    "- order by average tip percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef455ac8-7d1f-48ba-bdef-065eb3386cfe",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Import libraries\n",
    "- Setup Spark\n",
    "- Load dataset in memory\n",
    "- Setup helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8cce42a-5329-4886-865c-ef8ec554e2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/11 19:55:19 WARN Utils: Your hostname, rioly, resolves to a loopback address: 127.0.1.1; using 192.168.1.7 instead (on interface wlan0)\n",
      "25/11/11 19:55:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/11 19:55:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created.\n",
      "✅ Startup completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Tip Analysis - Python Version\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session created.\")\n",
    "\n",
    "# helpers\n",
    "def safe_hour(value):\n",
    "    try:\n",
    "        if isinstance(value, str) and len(value) >= 13:\n",
    "            return int(value[11:13])\n",
    "        elif isinstance(value, datetime):\n",
    "            return value.hour\n",
    "        else:\n",
    "            return -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def safe_long(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def tip_pct(fare, tip):\n",
    "    return (tip / fare) * 100.0 if fare > 0 else 0.0\n",
    "\n",
    "# Load data in memory\n",
    "trips_path = \"sample-data/yellow_tripdata_2022-01.parquet\"\n",
    "zones_path = \"sample-data/taxi_zone_lookup.csv\"\n",
    "output_path = \"output/results\"\n",
    "\n",
    "# Read parquet files\n",
    "df_trips = spark.read.parquet(trips_path).select(\n",
    "    col(\"PULocationID\").cast(LongType()).alias(\"PULocationID\"),\n",
    "    col(\"tpep_pickup_datetime\"),\n",
    "    col(\"fare_amount\"),\n",
    "    col(\"tip_amount\")\n",
    ")\n",
    "\n",
    "# Read zone lookup\n",
    "df_zones = spark.read.option(\"header\", True).csv(zones_path) \\\n",
    "    .select(\n",
    "        col(\"LocationID\").cast(LongType()).alias(\"LocationID\"),\n",
    "        col(\"Borough\"),\n",
    "        col(\"Zone\")\n",
    "    )\n",
    "\n",
    "print(\"✅ Startup completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e901de04-2c82-42ef-947f-126e4cf3cac0",
   "metadata": {},
   "source": [
    "# Main Job\n",
    "\n",
    "## Non optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d605c7-4633-4eea-9757-76565fdd7455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Non-optimized pipeline completed in 16.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "import time\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "zones_list = [(r[\"LocationID\"], (r[\"Borough\"], r[\"Zone\"])) for r in df_zones.collect()]\n",
    "zones_map = dict(zones_list)\n",
    "\n",
    "rdd_trips = df_trips.rdd.map(lambda r: (\n",
    "    safe_long(r[\"PULocationID\"]),\n",
    "    (r[\"tpep_pickup_datetime\"], float(r[\"fare_amount\"] or 0.0), float(r[\"tip_amount\"] or 0.0))\n",
    "))\n",
    "\n",
    "rdd_joined = rdd_trips.map(lambda x: (\n",
    "    (x[0], *zones_map.get(x[0], (\"Unknown\", \"Unknown\")), safe_hour(x[1][0])),\n",
    "    (tip_pct(x[1][1], x[1][2]), 1)\n",
    "))\n",
    "\n",
    "rdd_hour_agg = rdd_joined.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "                         .mapValues(lambda x: (x[0] / x[1], x[1]))\n",
    "\n",
    "rdd_zone_agg = rdd_hour_agg.map(lambda x: ((x[0][0], x[0][1], x[0][2]), (x[1][0] * x[1][1], x[1][1]))) \\\n",
    "                           .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "                           .map(lambda x: (x[0][0], x[0][1], x[0][2], x[1][0] / x[1][1], x[1][1]))\n",
    "\n",
    "top_zones_nonopt = rdd_zone_agg.sortBy(lambda x: x[3], ascending=False).take(20)\n",
    "\n",
    "df_result_nonopt = spark.createDataFrame(top_zones_nonopt, [\"PULocationID\", \"Borough\", \"Zone\", \"avg_tip_pct\", \"count\"])\n",
    "df_result_nonopt.coalesce(1).write.mode(\"overwrite\").json(output_path + \"_safe_non_optimized.json\")\n",
    "\n",
    "t_end = time.time()\n",
    "t_nonopt = t_end - t_start\n",
    "print(f\"⏱️ Non-optimized pipeline completed in {t_nonopt:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec394b8-4412-4b91-9196-3d398fe78dca",
   "metadata": {},
   "source": [
    "## Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f43e05-696d-4e41-a63e-6950ee220d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optimized pipeline completed in 15.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "\n",
    "zones_map = {r[\"LocationID\"]: (r[\"Borough\"], r[\"Zone\"]) for r in df_zones.collect()}\n",
    "b_zones = spark.sparkContext.broadcast(zones_map)\n",
    "\n",
    "rdd_trips = df_trips.rdd.map(lambda r: (\n",
    "    safe_long(r[\"PULocationID\"]),\n",
    "    (r[\"tpep_pickup_datetime\"], float(r[\"fare_amount\"] or 0.0), float(r[\"tip_amount\"] or 0.0))\n",
    "))\n",
    "\n",
    "rdd_enriched = rdd_trips.map(lambda x: (\n",
    "    (x[0], *b_zones.value.get(x[0], (\"Unknown\", \"Unknown\")), safe_hour(x[1][0])),\n",
    "    (tip_pct(x[1][1], x[1][2]), 1)\n",
    "))\n",
    "\n",
    "rdd_zone_hour_agg = rdd_enriched.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "                               .map(lambda x: ((x[0][0], x[0][1], x[0][2]), x[1][0] / x[1][1]))\n",
    "\n",
    "rdd_partitioned = rdd_zone_hour_agg.partitionBy(8).persist()\n",
    "\n",
    "rdd_zone_agg = rdd_partitioned.map(lambda x: (x[0], (x[1], 1))) \\\n",
    "                              .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "                              .map(lambda x: (x[0][0], x[0][1], x[0][2], x[1][0] / x[1][1], x[1][1]))\n",
    "\n",
    "top_zones_opt = rdd_zone_agg.sortBy(lambda x: x[3], ascending=False).take(20)\n",
    "\n",
    "df_result_opt = spark.createDataFrame(top_zones_opt, [\"PULocationID\", \"Borough\", \"Zone\", \"avg_tip_pct\", \"count\"])\n",
    "df_result_opt.coalesce(1).write.mode(\"overwrite\").json(output_path + \"_optimized.json\")\n",
    "\n",
    "t_end = time.time()\n",
    "t_opt = t_end - t_start\n",
    "print(f\"✅ Optimized pipeline completed in {t_opt:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1e15b-b386-44fc-8b9e-d5218db26b5e",
   "metadata": {},
   "source": [
    "## Timings comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06da6e82-59e2-48a2-876f-d9ba1400c42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARISON RESULTS ===\n",
      "Non-optimized time: 16.82s\n",
      "Optimized time:     15.26s\n",
      "Speedup:            1.10x\n",
      "Time saved:         1.56s\n"
     ]
    }
   ],
   "source": [
    "speedup = t_nonopt / t_opt\n",
    "saved = t_nonopt - t_opt\n",
    "\n",
    "print(\"=== COMPARISON RESULTS ===\")\n",
    "print(f\"Non-optimized time: {t_nonopt:.2f}s\")\n",
    "print(f\"Optimized time:     {t_opt:.2f}s\")\n",
    "print(f\"Speedup:            {speedup:.2f}x\")\n",
    "print(f\"Time saved:         {saved:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
